{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is used to test building the Graph Attention Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dependencies: Pytorch and LayerType constants from utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base model of a GAT layer containing initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayerBase(torch.nn.Module):\n",
    "    def __init__(self, num_in_features, num_out_features, num_of_heads, concat=True, activation=nn.ELU(),\n",
    "                 dropout_prob=0.6, add_skip_connection=True, bias=True, log_attention_weights=False):\n",
    "        super().__init__()\n",
    "        self.num_of_heads = num_of_heads\n",
    "        self.num_out_features = num_out_features\n",
    "        self.concat = concat  # attention heads aggregation method (concatenation/mean)\n",
    "        self.add_skip_connection = add_skip_connection\n",
    "\n",
    "        # These status below are trainable weights including the linear layer, \n",
    "        # attention matrices, bias and attention aggregation methods\n",
    "\n",
    "        # Linear projection: Specifying a linear layer have input data as a matrix containing \"num_in_features\" rows of features\n",
    "        # and the weights will be of the matrix of dimension (num_of_heads * num_out_features) * num_in_features\n",
    "        # (num_of_heads * num_out_features) because of multi-head attention mechanism so we have to multiply the num_of_heads\n",
    "        self.linear_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n",
    "\n",
    "        # Instead of doing [x, y] (concatenation, x/y are node feature vectors) and dot product with \"a\" (attention matrix)\n",
    "        # we instead do a dot product between x and \"a_left\" and y and \"a_right\" and we sum them up\n",
    "        self.scoring_fn_target = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n",
    "        self.scoring_fn_source = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n",
    "\n",
    "        if bias and concat:\n",
    "            self.bias = nn.Parameter(torch.Tensor(num_of_heads * num_out_features))\n",
    "        elif bias and not concat:\n",
    "            self.bias = nn.Parameter(torch.Tensor(num_out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        if add_skip_connection:\n",
    "            self.skip_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n",
    "        else:\n",
    "            self.register_parameter('skip_proj', None)\n",
    "\n",
    "        self.leakyReLU = nn.LeakyReLU(0.2)  # using 0.2 as in the paper of Veličković P. et al. (2018)\n",
    "        # Non-linearization for classifcation task: For our specific context, that is graph prediction\n",
    "        self.softmax = nn.Softmax(dim=-1)  # -1 stands for apply the log-softmax along the last dimension\n",
    "        self.activation = activation # chosen by user\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        self.log_attention_weights = log_attention_weights  # whether we should log the attention weights\n",
    "        self.attention_weights = None  # for later visualization purposes, I cache the weights here\n",
    "\n",
    "        self.init_params()\n",
    "    \n",
    "    def init_params(self, layer_type):\n",
    "        nn.init.xavier_uniform_(self.linear_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.scoring_fn_target)\n",
    "        nn.init.xavier_uniform_(self.scoring_fn_source)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            torch.nn.init.zeros_(self.bias)\n",
    "    \n",
    "    def skip_concat_bias(self, attention_coefficients, in_nodes_features, out_nodes_features):\n",
    "        if self.log_attention_weights:  # potentially log for later visualization in playground.py\n",
    "            self.attention_weights = attention_coefficients\n",
    "\n",
    "        # if the tensor is not contiguously stored in memory we'll get an error after we try to do certain ops like view\n",
    "        # only imp1 will enter this one\n",
    "        if not out_nodes_features.is_contiguous():\n",
    "            out_nodes_features = out_nodes_features.contiguous()\n",
    "\n",
    "        if self.add_skip_connection:  # add skip or residual connection\n",
    "            if out_nodes_features.shape[-1] == in_nodes_features.shape[-1]:  # if FIN == FOUT\n",
    "                out_nodes_features += in_nodes_features.unsqueeze(1)\n",
    "            else:\n",
    "                out_nodes_features += self.skip_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n",
    "\n",
    "        if self.concat:\n",
    "            # shape = (N, NH, FOUT) -> (N, NH*FOUT)\n",
    "            out_nodes_features = out_nodes_features.view(-1, self.num_of_heads * self.num_out_features)\n",
    "        else:\n",
    "            # shape = (N, NH, FOUT) -> (N, FOUT)\n",
    "            out_nodes_features = out_nodes_features.mean(dim=self.head_dim)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out_nodes_features += self.bias\n",
    "\n",
    "        return out_nodes_features if self.activation is None else self.activation(out_nodes_features)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specified GAT Layer referencing implementation of Gordic and theory of Veličković P."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gat():\n",
    "    src_nodes_dim = 0  # position of source nodes in edge index\n",
    "    trg_nodes_dim = 1  # position of target nodes in edge index\n",
    "\n",
    "    nodes_dim = 0      # node dimension/axis\n",
    "    head_dim = 1       # attention head dimension/axis\n",
    "\n",
    "    def __init__(self, num_in_features, num_out_features, num_of_heads, concat=True, activation=nn.ELU(),\n",
    "                 dropout_prob=0.6, add_skip_connection=True, bias=True, log_attention_weights=False):\n",
    "\n",
    "        # Delegate initialization to the base class\n",
    "        super().__init__(num_in_features, num_out_features, num_of_heads, concat, activation, dropout_prob,\n",
    "                      add_skip_connection, bias, log_attention_weights)\n",
    "\n",
    "    def fit_forward(self, data):\n",
    "        # Step 1: data linear projection + data regularization preprocessing\n",
    "\n",
    "        in_nodes_features, edge_index = data  # unpack data\n",
    "        num_of_nodes = in_nodes_features.shape[self.nodes_dim]\n",
    "        assert edge_index.shape[0] == 2, f'Expected edge index with shape=(2,E) got {edge_index.shape}'\n",
    "\n",
    "        # shape = (N, FIN) where N - number of nodes in the graph, FIN - number of input features per node\n",
    "        in_nodes_features = self.dropout(in_nodes_features)\n",
    "        nodes_features_proj = self.linear_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n",
    "        nodes_features_proj = self.dropout(nodes_features_proj)  # in the official GAT imp of Veličković P. et al., they did dropout here as well\n",
    "\n",
    "        # Step 2: Edge attention calculation\n",
    "\n",
    "        # Apply the scoring function (* represents element-wise (a.k.a. Hadamard) product)\n",
    "        # shape = (N, NH, FOUT) * (1, NH, FOUT) -> (N, NH, 1) -> (N, NH) because sum squeezes the last dimension\n",
    "        # Optimization note: torch.sum() is as performant as .sum() in my experiments\n",
    "        scores_source = (nodes_features_proj * self.scoring_fn_source).sum(dim=-1)\n",
    "        scores_target = (nodes_features_proj * self.scoring_fn_target).sum(dim=-1)\n",
    "\n",
    "        # We simply copy (lift) the scores for source/target nodes based on the edge index. Instead of preparing all\n",
    "        # the possible combinations of scores we just prepare those that will actually be used and those are defined\n",
    "        # by the edge index.\n",
    "        # scores shape = (E, NH), nodes_features_proj_lifted shape = (E, NH, FOUT), E - number of edges in the graph\n",
    "        scores_source_lifted, scores_target_lifted, nodes_features_proj_lifted = self.lift(scores_source, scores_target, nodes_features_proj, edge_index)\n",
    "        scores_per_edge = self.leakyReLU(scores_source_lifted + scores_target_lifted)\n",
    "\n",
    "        # shape = (E, NH, 1)\n",
    "        attentions_per_edge = self.neighborhood_attention_softmax(scores_per_edge, edge_index[self.trg_nodes_dim], num_of_nodes)\n",
    "        # Add stochasticity to neighborhood aggregation\n",
    "        attentions_per_edge = self.dropout(attentions_per_edge)\n",
    "\n",
    "        # Step 3: Neighborhood aggregation\n",
    "\n",
    "        # Element-wise (aka Hadamard) product. Operator * does the same thing as torch.mul\n",
    "        # shape = (E, NH, FOUT) * (E, NH, 1) -> (E, NH, FOUT), 1 gets broadcast into FOUT\n",
    "        nodes_features_proj_lifted_weighted = nodes_features_proj_lifted * attentions_per_edge\n",
    "\n",
    "        # This part sums up weighted and projected neighborhood feature vectors for every target node\n",
    "        # shape = (N, NH, FOUT)\n",
    "        out_nodes_features = self.aggregate_neighbors(nodes_features_proj_lifted_weighted, edge_index, in_nodes_features, num_of_nodes)\n",
    "\n",
    "        #\n",
    "        # Step 4: Residual/skip connections, concat and bias\n",
    "        #\n",
    "\n",
    "        out_nodes_features = self.skip_concat_bias(attentions_per_edge, in_nodes_features, out_nodes_features)\n",
    "        return (out_nodes_features, edge_index)\n",
    "\n",
    "    # Helper functions\n",
    "\n",
    "    def neighborhood_attention_softmax(self, scores_per_edge, trg_index, num_of_nodes):\n",
    "        scores_per_edge = scores_per_edge - scores_per_edge.max()\n",
    "        exp_scores_per_edge = scores_per_edge.exp()  # softmax\n",
    "\n",
    "        neigborhood_attention_denominator = self.sum_edge_scores_neighborhood_attention(exp_scores_per_edge, trg_index, num_of_nodes)\n",
    "        attentions_per_edge = exp_scores_per_edge / (neigborhood_attention_denominator + 1e-16)\n",
    "        return attentions_per_edge.unsqueeze(-1)\n",
    "\n",
    "    def sum_edge_scores_neighborhood_attention(self, exp_scores_per_edge, trg_index, num_of_nodes):\n",
    "        trg_index_broadcasted = self.explicit_broadcast(trg_index, exp_scores_per_edge)\n",
    "        size = list(exp_scores_per_edge.shape)  # convert to list otherwise assignment is not possible\n",
    "        size[self.nodes_dim] = num_of_nodes\n",
    "        neighborhood_sums = torch.zeros(size, dtype=exp_scores_per_edge.dtype, device=exp_scores_per_edge.device)\n",
    "        neighborhood_sums.scatter_add_(self.nodes_dim, trg_index_broadcasted, exp_scores_per_edge)\n",
    "        return neighborhood_sums.index_select(self.nodes_dim, trg_index)\n",
    "\n",
    "\n",
    "    def aggregate_neighbors(self, nodes_features_proj_lifted_weighted, edge_index, in_nodes_features, num_of_nodes):\n",
    "        size = list(nodes_features_proj_lifted_weighted.shape)\n",
    "        size[self.nodes_dim] = num_of_nodes\n",
    "        out_nodes_features = torch.zeros(size, dtype=in_nodes_features.dtype, device=in_nodes_features.device)\n",
    "        trg_index_broadcasted = self.explicit_broadcast(edge_index[self.trg_nodes_dim], nodes_features_proj_lifted_weighted)\n",
    "        out_nodes_features.scatter_add_(self.nodes_dim, trg_index_broadcasted, nodes_features_proj_lifted_weighted)\n",
    "        return out_nodes_features\n",
    "\n",
    "    def lift(self, scores_source, scores_target, nodes_features_matrix_proj, edge_index):\n",
    "        src_nodes_index = edge_index[self.src_nodes_dim]\n",
    "        trg_nodes_index = edge_index[self.trg_nodes_dim]\n",
    "        scores_source = scores_source.index_select(self.nodes_dim, src_nodes_index)\n",
    "        scores_target = scores_target.index_select(self.nodes_dim, trg_nodes_index)\n",
    "        nodes_features_matrix_proj_lifted = nodes_features_matrix_proj.index_select(self.nodes_dim, src_nodes_index)\n",
    "        return scores_source, scores_target, nodes_features_matrix_proj_lifted\n",
    "\n",
    "    def explicit_broadcast(self, this, other):\n",
    "        for _ in range(this.dim(), other.dim()):\n",
    "            this = this.unsqueeze(-1)\n",
    "        return this.expand_as(other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference:\n",
    "  1. {Gordić2020PyTorchGAT,<br>\n",
    "      author = {Gordić, Aleksa},<br>\n",
    "      title = {pytorch-GAT},<br>\n",
    "      year = {2020},<br>\n",
    "      publisher = {GitHub},<br>\n",
    "      journal = {GitHub repository},<br>\n",
    "      howpublished = {https://github.com/gordicaleksa/pytorch-GAT}<br>\n",
    "    }\n",
    "  2. Veličković P. et al. (2018) Graph Attention Networks. ICLR 2018. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thanks to the research of Causual Attention Learning (Wang X. et al., 2022)\n",
    "### We try building GAT but with Causual Attention Learning, extracting both causual attention and trivial attention and used causaul attention to predict Stroke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CALGAT Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import os\n",
    "from torch_geometric.nn import GATConv  # Import PyG's GATConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Define the GNNEncoder with Pytorch Geometry GATConv and CAL architecture diagram of Wang (2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    GNN encoder using PyG's GATConv\n",
    "    \"\"\"\n",
    "    def __init__(self, num_in_features, num_hidden_features, num_of_heads, dropout_prob, alpha):\n",
    "        super(GNNEncoder, self).__init__()\n",
    "        self.gat1 = GATConv(num_in_features, num_hidden_features, heads=num_of_heads, dropout=dropout_prob, negative_slope=alpha)\n",
    "        self.gat2 = GATConv(num_hidden_features * num_of_heads, num_hidden_features, heads=num_of_heads, dropout=dropout_prob, negative_slope=alpha) # Add another GAT layer if needed\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = F.dropout(x, p=0.6, training=self.training) # Apply dropout to input features\n",
    "        h = self.gat1(h, edge_index)\n",
    "        h = F.elu(h) # Use ELU activation as in original GAT paper (Veličković P. et al., 2018)\n",
    "        h = F.dropout(h, p=0.6, training=self.training) # Apply dropout after first GAT layer\n",
    "        h = self.gat2(h, edge_index) # Add second GAT layer\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Define Node Attention Functions and Edge Attention Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeAttention(nn.Module):\n",
    "    \"\"\"Node-level attention module to separate causal and trivial features\"\"\"\n",
    "    def __init__(self, input_dim, dropout_prob=0.6):\n",
    "        super(NodeAttention, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(input_dim // 2, 2)  # 2 outputs: causal and trivial\n",
    "        )\n",
    "\n",
    "    def forward(self, node_features):\n",
    "        # Compute attention scores [batch_size, 2]\n",
    "        scores = self.mlp(node_features)\n",
    "        attention = F.softmax(scores, dim=1)\n",
    "        # Split into causal and trivial attention\n",
    "        node_c = attention[:, 0] # nodes attention scores causal\n",
    "        node_t = attention[:, 1] # nodes attention scores trivial\n",
    "        return node_c, node_t\n",
    "\n",
    "\n",
    "class EdgeAttention(nn.Module):\n",
    "    \"\"\"Edge-level attention for causal and trivial connections\"\"\"\n",
    "    def __init__(self, input_dim, dropout_prob=0.6):\n",
    "        super(EdgeAttention, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim * 2, input_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(input_dim, 2)  # 2 outputs: causal and trivial\n",
    "        )\n",
    "\n",
    "    def forward(self, node_features, edge_index):\n",
    "        # Get features for each edge\n",
    "        src_features = node_features[edge_index[0]]  # source features [num_edges, num_dim]\n",
    "        dst_features = node_features[edge_index[1]]  # destination features [num_edges, num_dim]\n",
    "        # Concatenate source and destination features\n",
    "        edge_features = torch.cat([src_features, dst_features], dim=1)  # [num_edges, 2*input_dim]\n",
    "        # Compute attention scores\n",
    "        scores = self.mlp(edge_features)  # [num_edges, 2]\n",
    "        attention = F.softmax(scores, dim=1)\n",
    "        # Split into causal and trivial attention\n",
    "        edge_c = attention[:, 0]  # [num_edges]\n",
    "        edge_t = attention[:, 1]  # [num_edges]\n",
    "        return edge_c, edge_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Define Graph Convolutional Layer with Pytorch Geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConv(nn.Module):\n",
    "    \"\"\"Graph convolution layer for processing attended graphs\"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GraphConv, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.edge_proj = nn.Linear(1, 1, bias=False)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        # Basic graph convolution with optional edge weights\n",
    "        out = self.linear(x)\n",
    "        # If using edge weights, apply them during message passing (simplified)\n",
    "        if edge_attr is not None:\n",
    "            edge_weights = self.edge_proj(edge_attr.view(-1, 1)).view(-1)\n",
    "            src, dst = edge_index\n",
    "            for i in range(len(src)):\n",
    "                out[dst[i]] += edge_weights[i] * x[src[i]]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Alternative Scatter Mean function for torch_scatter.scatter_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_mean_alternative(gated_x, batch):\n",
    "    \"\"\"\n",
    "    Alternative implementation for torch_scatter.scatter_mean\n",
    "    without using torch_scatter.\n",
    "\n",
    "    Args:\n",
    "        gated_x: Tensor of node features (e.g., [N, features]).\n",
    "        batch: Batch assignment vector (e.g., [N]).\n",
    "\n",
    "    Returns:\n",
    "        Tensor of mean node features per batch (e.g., [num_batches, features]).\n",
    "    \"\"\"\n",
    "    if batch is None:\n",
    "        return gated_x.mean(dim=0, keepdim=True)  # Return mean over all nodes if no batch info\n",
    "\n",
    "    unique_batches = torch.unique(batch)\n",
    "    batch_means = []\n",
    "    for b_idx in unique_batches:\n",
    "        mask = (batch == b_idx)\n",
    "        current_batch_nodes = gated_x[mask]\n",
    "        batch_mean = current_batch_nodes.mean(dim=0) # Mean across nodes within this batch\n",
    "        batch_means.append(batch_mean)\n",
    "\n",
    "    return torch.stack(batch_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Define Readout Function (similar to the official GAT architecture proposed by Petar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadoutFunction(nn.Module):\n",
    "    \"\"\"Readout function for graph-level representations\"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super(ReadoutFunction, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, input_dim)\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, batch=None):\n",
    "        # Gate mechanism for readout\n",
    "        gate = self.gate(x)\n",
    "        gated_x = x * gate\n",
    "        # If batch is provided, use it to aggregate node features\n",
    "        if batch is not None:\n",
    "            return scatter_mean_alternative(gated_x, batch)\n",
    "        # For a single graph, just mean over all nodes\n",
    "        return torch.mean(gated_x, dim=0, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 5: Define Classifier for graph classification based on CAL paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \"\"\"Classifier for graph classification\"\"\"\n",
    "    def __init__(self, in_features, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features, in_features // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "class CAL_GAT(nn.Module):\n",
    "    \"\"\"Graph Attention Network with Causal Attention Learning\"\"\"\n",
    "    def __init__(self, num_in_features, num_hidden_features, num_out_features, num_of_heads=8,\n",
    "                 dropout_prob=0.6, alpha=0.2, lambda1=0.1, lambda2=0.1):\n",
    "        super(CAL_GAT, self).__init__()\n",
    "\n",
    "        # GNN Encoder with PyG's GATConv\n",
    "        self.gnn_encoder = GNNEncoder(num_in_features, num_hidden_features, num_of_heads, dropout_prob, alpha)\n",
    "\n",
    "        # Calculate feature dimension after encoder\n",
    "        hidden_dim = num_hidden_features # After GNNEncoder, hidden_dim is just num_hidden_features (due to layer design, not num_hidden_features * num_heads as before)\n",
    "\n",
    "        # Node and edge attention\n",
    "        self.node_attention = NodeAttention(hidden_dim, dropout_prob)\n",
    "        self.edge_attention = EdgeAttention(hidden_dim, dropout_prob)\n",
    "\n",
    "        # GraphConv layers for causal and trivial branches\n",
    "        self.graph_conv_causal = GraphConv(hidden_dim, hidden_dim)\n",
    "        self.graph_conv_trivial = GraphConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Readout functions\n",
    "        self.readout_causal = ReadoutFunction(hidden_dim)\n",
    "        self.readout_trivial = ReadoutFunction(hidden_dim)\n",
    "\n",
    "        # Classifiers\n",
    "        self.classifier_causal = Classifier(hidden_dim, num_out_features)\n",
    "        self.classifier_trivial = Classifier(hidden_dim, num_out_features)\n",
    "        self.classifier_combined = Classifier(hidden_dim * 2, num_out_features) # Combined classifier input dim adjusted\n",
    "\n",
    "        # Hyperparameters for loss functions\n",
    "        self.lambda1 = lambda1  # Controls disentanglement strength\n",
    "        self.lambda2 = lambda2  # Controls causal intervention strength\n",
    "\n",
    "    def forward(self, data, shuffled_h_G_t=None): # shuffled_h_G_t is now an argument for causal intervention\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        edge_attr = data.edge_attr if hasattr(data, 'edge_attr') else None\n",
    "        batch = data.batch if hasattr(data, 'batch') else None\n",
    "\n",
    "        # Step 1: Encode graph with GNN\n",
    "        H = self.gnn_encoder(x, edge_index)\n",
    "\n",
    "        # Step 2: Compute node and edge attention scores\n",
    "        alpha_c, alpha_t = self.node_attention(H)  # Node-level attention scores\n",
    "        beta_c, beta_t = self.edge_attention(H, edge_index)  # Edge-level attention scores\n",
    "\n",
    "        # Step 3: Create attended representations\n",
    "        # Node masking\n",
    "        H_c = H * alpha_c.unsqueeze(1)  # Causal attended nodes\n",
    "        H_t = H * alpha_t.unsqueeze(1)  # Trivial attended nodes\n",
    "\n",
    "        # Edge masking - if you have edge attributes to mask, do it here. If masking adjacency directly, it is more complex in PyG and might be out of scope for this step for now.\n",
    "        edge_attr_c = edge_attr * beta_c.unsqueeze(1) if edge_attr is not None else None # beta_c as edge weights for causal\n",
    "        edge_attr_t = edge_attr * beta_t.unsqueeze(1) if edge_attr is not None else None # beta_t as edge weights for trivial\n",
    "\n",
    "        # Step 4: Process through GraphConv layers\n",
    "        G_c = self.graph_conv_causal(H_c, edge_index, edge_attr_c)\n",
    "        G_t = self.graph_conv_trivial(H_t, edge_index, edge_attr_t)\n",
    "\n",
    "        # Step 5: Readout functions to get graph-level representations\n",
    "        h_G_c = self.readout_causal(G_c, batch)\n",
    "        h_G_t = self.readout_trivial(G_t, batch)\n",
    "\n",
    "        # Step 6: Predictions (causal and trivial branches)\n",
    "        z_G_c = self.classifier_causal(h_G_c)  # Causal prediction\n",
    "        z_G_t = self.classifier_trivial(h_G_t)  # Trivial prediction\n",
    "\n",
    "        # Step 7: Causal intervention and combined prediction\n",
    "        if shuffled_h_G_t is not None: # During training, shuffled_h_G_t will be provided\n",
    "            h_G_combined = h_G_c + shuffled_h_G_t # Using addition as in Algorithm 1, step 18. You can change to concatenation if preferred: torch.cat([h_G_c, shuffled_h_G_t], dim=1)\n",
    "            z_G_prime = self.classifier_combined(h_G_combined) # Classifier on combined representation\n",
    "        else: # During inference, no shuffled trivial features, so use combined features from same graph (like before)\n",
    "            z_G_prime = self.classifier_combined(torch.cat([h_G_c, h_G_t], dim=1)) # Fallback to combined from same graph if no shuffled trivial features.\n",
    "\n",
    "        return z_G_c, z_G_t, z_G_prime, h_G_c, h_G_t\n",
    "\n",
    "    def compute_losses(self, z_G_c, z_G_t, z_G_prime, labels, num_classes):\n",
    "        \"\"\"Compute CAL losses\"\"\"\n",
    "        # Supervised loss for causal branch\n",
    "        if len(labels.shape) == 1 or labels.shape[1] == 1:\n",
    "            L_sup = F.cross_entropy(z_G_c, labels.view(-1))\n",
    "        else:\n",
    "            L_sup = F.binary_cross_entropy_with_logits(z_G_c, labels.float())\n",
    "\n",
    "        # Uniform classification loss for trivial branch\n",
    "        uniform_target = torch.ones(z_G_t.size(0), num_classes).to(z_G_t.device) # Corrected uniform target shape\n",
    "        L_unif = F.kl_div(F.log_softmax(z_G_t, dim=1), uniform_target, reduction='batchmean')\n",
    "\n",
    "        # Causal intervention loss\n",
    "        if len(labels.shape) == 1 or labels.shape[1] == 1:\n",
    "            L_caus = F.cross_entropy(z_G_prime, labels.view(-1))\n",
    "        else:\n",
    "            L_caus = F.binary_cross_entropy_with_logits(z_G_prime, labels.float())\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = L_sup + self.lambda1 * L_unif + self.lambda2 * L_caus\n",
    "        return total_loss, L_sup, L_unif, L_caus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### End of CALGAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Execution and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading data from: path/to/your/local/data\n",
      "Error loading data: [Errno 2] No such file or directory: 'path/to/your/local/data\\\\labels.npy'\n"
     ]
    }
   ],
   "source": [
    "def prepare_graph_data(adjacency_matrix, covariance_features):\n",
    "    \"\"\"Convert adjacency matrix and covariance features to PyTorch Geometric format\"\"\"\n",
    "    # Get edges from adjacency matrix (where weight > 0)\n",
    "    edges = np.where(adjacency_matrix > 0)\n",
    "    edge_index = torch.tensor(np.vstack((edges[0], edges[1])), dtype=torch.long)\n",
    "    # Get edge weights from adjacency matrix\n",
    "    edge_attr = torch.tensor(adjacency_matrix[edges], dtype=torch.float)\n",
    "    # Process covariance features\n",
    "    num_nodes = covariance_features.shape[0]\n",
    "    feature_dim = covariance_features.shape[1]\n",
    "    # Extract meaningful features from covariance matrices\n",
    "    node_features = []\n",
    "    for i in range(num_nodes):\n",
    "        covar = covariance_features[i]\n",
    "        # 1. Diagonal elements (variances)\n",
    "        diag_features = np.diag(covar)\n",
    "        # 2. Upper triangular part (correlations)\n",
    "        triu_indices = np.triu_indices(feature_dim, k=1)\n",
    "        triu_features = covar[triu_indices]\n",
    "        # 3. Combine features\n",
    "        node_feat = np.concatenate([diag_features, triu_features])\n",
    "        node_features.append(node_feat)\n",
    "    x = torch.tensor(np.stack(node_features), dtype=torch.float)\n",
    "    # Create PyTorch Geometric Data object\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    return data\n",
    "\n",
    "\n",
    "def prepare_dataset(adjacency_matrices, covariance_features_list, labels):\n",
    "    \"\"\"Prepare dataset for multiple graphs\"\"\"\n",
    "    dataset = []\n",
    "    for i, (adj, cov) in enumerate(zip(adjacency_matrices, covariance_features_list)):\n",
    "        data = prepare_graph_data(adj, cov)\n",
    "        # Add graph label\n",
    "        data.y = torch.tensor([labels[i]], dtype=torch.long)\n",
    "        dataset.append(data)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_local_data(data_path):\n",
    "    \"\"\"\n",
    "    Load adjacency matrices, covariance features, and labels from local files.\n",
    "    Assumes files are in .npy format and named as:\n",
    "    - adj_matrix_graph_i.npy: Adjacency matrix for graph i\n",
    "    - covar_features_graph_i.npy: Covariance features for graph i\n",
    "    - labels.npy:  Numpy array of labels for all graphs\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_path : str\n",
    "        Path to the directory containing the data files.\n",
    "    Returns:\n",
    "    --------\n",
    "    adjacency_matrices : list\n",
    "        List of adjacency matrices (numpy arrays).\n",
    "    covariance_features_list : list\n",
    "        List of covariance features (numpy arrays).\n",
    "    labels : numpy.ndarray\n",
    "        Numpy array of graph labels.\n",
    "    \"\"\"\n",
    "    adjacency_matrices = []\n",
    "    covariance_features_list = []\n",
    "    # Load labels\n",
    "    labels_path = os.path.join(data_path, 'labels.npy')\n",
    "    labels = np.load(labels_path)\n",
    "    graph_index = 0 # Start index for graph files\n",
    "    while True: # Try loading files until no more are found\n",
    "        adj_matrix_file = os.path.join(data_path, f'adj_matrix_graph_{graph_index}.npy')\n",
    "        covar_features_file = os.path.join(data_path, f'covar_features_graph_{graph_index}.npy')\n",
    "        if not os.path.exists(adj_matrix_file) or not os.path.exists(covar_features_file):\n",
    "            break # Stop loading if files for the current graph index are not found\n",
    "        adj_matrix = np.load(adj_matrix_file)\n",
    "        covar_features = np.load(covar_features_file)\n",
    "        adjacency_matrices.append(adj_matrix)\n",
    "        covariance_features_list.append(covar_features)\n",
    "        graph_index += 1 # Increment for next graph\n",
    "    if not adjacency_matrices: # Check if any data was loaded\n",
    "        raise FileNotFoundError(f\"No adjacency or covariance feature files found in: {data_path}. \"\n",
    "                                f\"Make sure files are named as 'adj_matrix_graph_i.npy' and \"\n",
    "                                f\"'covar_features_graph_i.npy' and 'labels.npy' are in the directory.\")\n",
    "    return adjacency_matrices, covariance_features_list, labels\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Training and evaluation functions (modified for causal intervention)\n",
    "# ------------------------------------------------------------------------\n",
    "def train_cal_gat(model, train_loader, optimizer, device, num_classes):\n",
    "    \"\"\"Train the CAL_GAT model for one epoch with causal intervention\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Step 1-6: Forward pass to get causal and trivial graph representations\n",
    "        z_G_c_batch, z_G_t_batch, _, h_G_c_batch, h_G_t_batch = model(batch) # No shuffled trivial features yet\n",
    "\n",
    "        # Step 7 & 8: Causal Intervention - Random combination across the batch\n",
    "        shuffled_indices = torch.randperm(batch.num_graphs) # Shuffle indices for trivial graph representations in the batch\n",
    "        shuffled_h_G_t_batch = h_G_t_batch[shuffled_indices] # Shuffle trivial graph representations\n",
    "\n",
    "        # Step 7 (continued) and Predictions with causal intervention\n",
    "        z_G_c, z_G_t, z_G_prime, _, _ = model(batch, shuffled_h_G_t=shuffled_h_G_t_batch) # Pass shuffled trivial features for combined prediction\n",
    "\n",
    "        # Compute losses\n",
    "        loss, L_sup, L_unif, L_caus = model.compute_losses(\n",
    "            z_G_c, z_G_t, z_G_prime, batch.y, num_classes\n",
    "        )\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * batch.num_graphs\n",
    "\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate_cal_gat(model, loader, device, num_classes):\n",
    "    \"\"\"Evaluate the CAL_GAT model (no causal intervention during evaluation)\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Forward pass (no shuffled trivial features in eval)\n",
    "            z_G_c, z_G_t, z_G_prime, h_G_c, h_G_t = model(batch) # shuffled_h_G_t=None by default\n",
    "\n",
    "            # Compute losses\n",
    "            loss, _, _, _ = model.compute_losses(\n",
    "                z_G_c, z_G_t, z_G_prime, batch.y, num_classes\n",
    "            )\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "\n",
    "            # Get predictions (using causal branch prediction for evaluation)\n",
    "            preds = z_G_c.argmax(dim=1).cpu().numpy() # Evaluate based on causal branch, or z_G_prime if you want to evaluate combined.\n",
    "            labels = batch.y.cpu().numpy().flatten()\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return total_loss / len(loader.dataset), accuracy, all_preds, all_labels\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Main execution\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Device setup\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Parameters\n",
    "    num_classes = 3\n",
    "    hidden_dim = 32\n",
    "    heads = 8 # Increased heads as GATConv is used now\n",
    "    dropout = 0.6 # Increased dropout as in original GAT paper\n",
    "    alpha = 0.2 # Alpha for LeakyReLU in GATConv\n",
    "    learning_rate = 0.005\n",
    "    epochs = 50\n",
    "    batch_size = 32\n",
    "\n",
    "    # --- Load data from local files ---\n",
    "    data_path = 'path/to/your/local/data'\n",
    "    print(f\"Loading data from: {data_path}\")\n",
    "    try:\n",
    "        adj_matrices, covar_features, labels = load_local_data(data_path)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Prepare dataset\n",
    "    print(\"Preparing dataset...\")\n",
    "    dataset = prepare_dataset(adj_matrices, covar_features, labels)\n",
    "\n",
    "    # Get feature dimensions\n",
    "    input_dim = dataset[0].x.size(1)\n",
    "    print(f\"Input feature dimension: {input_dim}\")\n",
    "\n",
    "    # 3. Split data into train/val/test\n",
    "    train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "    train_dataset, val_dataset = train_test_split(train_dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "    print(f\"Train set: {len(train_dataset)}, Validation set: {len(val_dataset)}, Test set: {len(test_dataset)}\")\n",
    "\n",
    "    # 4. Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # 5. Initialize the model\n",
    "    print(\"Initializing CAL_GAT model...\")\n",
    "    model = CAL_GAT(\n",
    "        num_in_features=input_dim,\n",
    "        num_hidden_features=hidden_dim,\n",
    "        num_out_features=num_classes,\n",
    "        num_of_heads=heads,\n",
    "        dropout_prob=dropout,\n",
    "        alpha=alpha,\n",
    "        lambda1=0.5,  # Weight for uniform loss\n",
    "        lambda2=1.0  # Weight for causal intervention loss\n",
    "    ).to(device)\n",
    "\n",
    "    # 6. Initialize optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-5\n",
    "    )\n",
    "\n",
    "    # 7. Training loop\n",
    "    print(\"Starting training...\")\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    best_val_acc = 0\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        train_loss = train_cal_gat(model, train_loader, optimizer, device, num_classes)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validate\n",
    "        val_loss, val_acc, _, _ = evaluate_cal_gat(model, val_loader, device, num_classes)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model = model.state_dict().copy()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # 8. Load best model and evaluate on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    model.load_state_dict(best_model)\n",
    "    test_loss, test_acc, test_preds, test_labels = evaluate_cal_gat(model, test_loader, device, num_classes)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    # 9. Confusion matrix\n",
    "    cm = confusion_matrix(test_labels, test_preds)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # 10. Plot training curves\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Losses')\n",
    "    plt.legend() # Add legend to the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
